# HIDA Training Configuration
# =========================
# Unified configuration file for HIDA training pipeline
# Supports both regular HIDA and HIDA with Objaverse datasets

# Dataset Generation Configuration
# ================================
dataset_generation:
  # Dataset type options:
  # - "hida": Uses create_csv.py (Shapegen + Primigen only)
  # - "hida_objaverse": Uses create_csv_w_Objaverse.py (Shapegen + Primigen + Objaverse)
  # - "hida_co3d": Uses create_csv_w_Objaverse_co3D.py (Shapegen + Primigen + CO3D)
  # - "hida_co3d_objaverse": Uses create_csv_w_Objaverse_co3D.py (Shapegen + Primigen + CO3D + Objaverse)
  # - "objaverse": Uses create_csv_w_Objaverse_co3D.py (Objaverse only)
  # - "co3d": Uses create_csv_w_Objaverse_co3D.py (CO3D only)
  # - "objaverse_co3d": Uses create_csv_w_Objaverse_co3D.py (Objaverse + CO3D only)
  dataset_type: "hida"
  
  # Paths for dataset generation
  paths:
    # Shapegen paths
    shapegen_black: "/datasets/hida/current/shapegen/images_black_bg"
    shapegen_random: "/datasets/hida/current/shapegen/images_random_bg" 
    shapegen_white: "/datasets/hida/current/shapegen/images_white_bg"
    shapegen_black_sim: "/datasets/hida/current/shapegen/obj_pair_sim_black_bg/object_similarities_precomputed.pkl"
    shapegen_random_sim: "/datasets/hida/current/shapegen/obj_pair_sim/object_similarities_precomputed.pkl"
    shapegen_white_sim: "/datasets/hida/current/shapegen/obj_pair_sim_white_bg/object_similarities_precomputed.pkl"
    shapegen_camera: "/datasets/hida/current/shapegen/base_images_camera_info"
    
    # Primigen paths
    primigen_black: "/datasets/hida/current/primigen/images_black_bg"
    primigen_random: "/datasets/hida/current/primigen/images_random_bg"
    primigen_white: "/datasets/hida/current/primigen/images_white_bg"
    primigen_camera: "/datasets/hida/current/primigen/base_images_camera_info"
    
    # Objaverse paths (used only when dataset_type includes "objaverse")
    objaverse_black: "/datasets/hida/current/Objaverse_segmented_black"
    objaverse_random: "/datasets/hida/current/Objaverse_segmented_random"
    objaverse_white: "/datasets/hida/current/Objaverse_segmented_white"
    objaverse_sim: "/datasets/hida/current/Objaverse_sim/object_similarities_objaverse.pkl"

    # CO3D paths (used only when dataset_type includes "co3d")
    co3d_black: "/datasets/hida/current/co3D_segmented_black"
    co3d_random: "/datasets/hida/current/co3D_segmented_random"
    co3d_white: "/datasets/hida/current/co3D_segmented_white"
    co3d_sim: "/datasets/hida/current/co3D_sim/sequence_similarities_co3d.pkl"
  
  # Generation parameters
  triplets_per_combo: 6666  # Number of triplets per dataset+background combination
  regenerate_per_epoch: true  # Regenerate dataset for each epoch (for data diversity)
  
  # Ratio distributions
  shapegen_ratios: [0.15, 0.55, 0.30]  # [low, medium, high] similarity ratios
  primigen_ratios: [0.60, 0.25, 0.15]  # [place, warp, config] condition ratios
  #shapegen_ratios: [0.05, 0.05, 0.90]  # [low, medium, high] similarity ratios
  #primigen_ratios: [0.90, 0.05, 0.05]  # [place, warp, config] condition ratios
  objaverse_ratios: [0.05, 0.20, 0.75]  # [low, medium, high] similarity ratios
  co3d_ratios: [0.20, 0.45, 0.35]      # [low, medium, high] similarity ratios
  n_ratios: [0.40, 0.40, 0.20]         # [n2, n3, n4] complexity ratios for Primigen
  
  # Random seed for reproducible dataset generation
  seed: 42

# System Configuration
# ===================
system:
  random_seed: 42
  num_gpus: 1

# Training Configuration
# =====================
training:
  # Training mode: "contrastive" or "siamese"
  # - contrastive: Standard triplet-based training (current approach)
  # - siamese: Binary similarity learning with frozen DINO backbone
  mode: "contrastive"

  batch_size: 32
  epochs: 30
  num_workers: 4
  log_dir: "./logs"

  # Siamese-specific configuration
  siamese:
    freeze_backbone: true  # Freeze DINO backbone during Siamese training
    dropout: 0.2  # Dropout for Siamese network
    lr: 2e-6  # Learning rate for Siamese network head (backbone uses optimizer.lr when unfrozen)
    wandb_project: "Siamese-learning"  # WandB project for Siamese experiments

    # Optimizer configuration for Siamese mode
    optimizer_type: "adam"  # Options: "adam", "adamw"

    # Scheduler configuration for Siamese mode
    use_scheduler: false  # Enable/disable learning rate scheduler
    warmup_epochs: 3  # Number of warmup epochs (linear warmup from 0.1*lr to lr)
    # After warmup, cosine annealing is applied for remaining epochs

# Model Configuration
# ==================
model:
  # Backbone configuration
  backbone:
    _target_: models.dinov2.DINOv2
    checkpoint: "vit_large_patch14_reg4_dinov2"  # Options: vit_base_patch14_dinov2, vit_large_patch14_dinov2
    output: "cls"
    layer: -1
    return_multilayer: false
    layer_norm_post: true
    image_size: 224
  
  # LoRA Configuration
  use_lora: true
  lora_r: 16
  lora_alpha: 8
  lora_dropout: 0.1

# Loss Configuration
# =================
loss:
  # Loss function type: "triplet", "hinge", "multi_similarity", "oddity", "pairwise_contrastive"
  # - triplet/hinge/multi_similarity/oddity: Operate on triplets (A, A', B)
  # - pairwise_contrastive: Operates on pairs (img1, img2, label) - automatically converts triplets to pairs
  type: "pairwise_contrastive"

  # Loss-specific parameters
  margin: 0.1  # Used by triplet and hinge loss
  temperature: 0.3  # Used by oddity loss (0.5 for pairwise_contrastive)

  # Multi-similarity loss parameters (used when type="multi_similarity")
  alpha: 2
  beta: 50
  base: 1

# Optimizer Configuration
# ======================
optimizer:
  _target_: torch.optim.Adam
  lr: 2e-6
  weight_decay: 1e-4
  betas: [0.9, 0.999]

# Dataset Configuration
# ====================
dataset:
  # Main dataset configuration
  _target_: datasets.hida_dataset.HIDADataset
  csv_path: "/datasets/hida/current/hida_dataset.csv"  # Will be dynamically set based on generation
  train_ratio: 0.95
  random_seed: 42
  split: null  # Base split configuration, overridden by train/val/test datasets
  filters: null  # Base filters configuration, overridden by train/val/test datasets

# Validation split configuration
# ==============================
validation:
  mode: "fixed"  # Options: "ratio" or "fixed"
  # - "ratio": Use dataset.train_ratio to determine split (e.g., 0.95 = 95% train, 5% val)
  # - "fixed": Use fixed number of validation samples
  fixed_size: 500  # Number of samples for validation when mode="fixed"

# Dataset splits configuration
# ============================
splits:
  train_dataset:
    split: "train"
    filters: null  # Can be used to filter by BG, DATASET, etc. Example: {BG: "BLACK", DATASET: "SHAPEGEN"}
  
  val_dataset:
    split: "val"
    filters: null
  
  test_dataset:
    _target_: datasets.mochi_dataset.MochiDataset
    csv_path: "/datasets/hida/current/neurips_benchmark/benchmark.csv"
    split: null
    sample_percentage: ${evaluation.mochi_sampling.sample_percentage}
    sample_seed: ${evaluation.mochi_sampling.seed}

# Data Augmentation Configuration
# ==============================
augmentation:
  # Enable/disable augmentations during training
  enabled: true
  
  # Augmentation parameters
  save_every_n: 1000000  # Save augmented samples every N items for inspection
  save_dir: "./augmented_samples"  # Directory to save augmented samples
  
  # Individual augmentation parameters
  color_jitter:
    brightness: 0.10
    contrast: 0.13
    saturation: 0.1
    hue: 0.05
    probability: 0.5
  
  gaussian_blur:
    kernel_size: 3
    sigma: [0.1, 0.6]
    probability: 0.5
  
  motion_blur:
    kernel_size: 5
    probability: 0.5
  
  gaussian_noise:
    mean: 0.0
    std_min: 0.003
    std_max: 0.03
    probability: 0.5

# Evaluation Configuration
# =======================
evaluation:
  batch_size: 70  # Batch size for evaluation
  num_workers: 4

  # Control which evaluations to run
  # You can enable/disable each evaluation independently
  run_mochi_cosine: true    # MOCHI cosine similarity (zero-shot)
  run_mochi_svm: false       # MOCHI SVM same-different classification
  run_imagenet: true        # ImageNet k-NN and/or linear probe (see imagenet_eval section)

  # MOCHI sampling configuration (percentage of each category to use)
  mochi_sampling:
    enabled: true  # Set to true to enable sampling
    sample_percentage: 0.5  # Percentage of each category to sample (0.0 to 1.0)
    seed: 42  # Random seed for reproducible sampling

# ImageNet Evaluation Configuration
# =================================
imagenet_eval:
  # Enable/disable ImageNet evaluation during training
  enabled: true

  # Path to ImageNet-22k dataset
  root_dir: "/datasets/imagenet_22k/current"

  # Evaluation settings
  eval_frequency: 1  # Evaluate every N epochs (set to 1 for every epoch)
  eval_knn: true  # Enable k-NN evaluation
  eval_linear: false  # Enable linear probe evaluation (more expensive)
  knn_k: 20  # Number of neighbors for k-NN

  # Dataset subset configuration (to speed up evaluation)
  max_classes: 200  # Maximum number of classes to use (ImageNet-22k has ~21k classes)
  max_samples_per_class_train: 100  # Maximum samples per class for training set
  max_samples_per_class_test: 50  # Maximum samples per class for test set
  train_ratio: 0.7  # Ratio of images to use for train vs test (within each class)

  # DataLoader settings
  batch_size: 128  # Batch size for feature extraction
  num_workers: 4  # Number of workers for data loading